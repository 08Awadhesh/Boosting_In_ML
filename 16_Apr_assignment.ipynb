{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f5fd2a-cae6-4495-86f7-828cbd59bd62",
   "metadata": {},
   "source": [
    "**Q1**. What is boosting in machine learning?\n",
    "\n",
    "**Answer**:\n",
    "Boosting is a machine learning ensemble technique used to improve the performance of weak learners, such as decision trees or simple models, by combining them into a strong learner. The main idea behind boosting is to train multiple weak models sequentially, where each subsequent model tries to correct the errors made by its predecessors.\n",
    "\n",
    "The boosting process works as follows:\n",
    "\n",
    "**(I) Initialization:** Each data point in the training set is assigned an equal weight initially.\n",
    "\n",
    "**(II) Training weak learners**: A weak learner (e.g., decision tree) is trained on the weighted training data. It aims to focus on the data points that were misclassified by the previous weak learners.\n",
    "\n",
    "**(III) Weight update**: After training a weak learner, the weights of the misclassified data points are increased so that they have a higher influence on the next iteration. The correctly classified data points may have their weights reduced.\n",
    "\n",
    "**(IV) Reiteration:** Steps 2 and 3 are repeated for a predefined number of iterations (or until a certain threshold is reached).\n",
    "\n",
    "**(V) Aggregation:** The final prediction is made by combining the predictions of all weak learners, where each learner's contribution is weighted based on its performance during training.\n",
    "\n",
    "The most popular boosting algorithm is AdaBoost (Adaptive Boosting). Other popular boosting algorithms include Gradient Boosting Machines (GBM) and Extreme Gradient Boosting (XGBoost).\n",
    "\n",
    "Boosting can significantly improve the performance of models, reduce overfitting, and handle complex relationships within the data. However, it is important to be cautious of potential model overfitting, which can happen if the boosting process is carried out for too many iterations or if the weak learners are too complex. Regularization techniques and early stopping are often used to mitigate overfitting in boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f61505-d31c-4f00-928f-fd46b67f0009",
   "metadata": {},
   "source": [
    "**Q2**. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "**Answer**:\n",
    "Boosting techniques offer several advantages that make them popular in machine learning applications. However, they also come with certain limitations that need to be considered. Let's explore both the advantages and limitations of using boosting techniques:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "**(I) Improved performance**: Boosting can significantly improve the predictive performance of weak learners, leading to more accurate and robust models compared to using individual weak learners alone.\n",
    "\n",
    "**(II) Versatility:** Boosting is a versatile technique that can be applied to various types of weak learners, such as decision trees, linear models, or neural networks, making it adaptable to different types of data and problems.\n",
    "\n",
    "**(III) Handling complex relationships:** Boosting can effectively capture complex relationships within the data, allowing it to model non-linear interactions between features.\n",
    "\n",
    "**(IV) Robustness to noise**: By iteratively focusing on misclassified data points, boosting can reduce the impact of noisy data and outliers, making the model more robust.\n",
    "\n",
    "**(V) Feature importance:** Boosting provides a measure of feature importance, indicating which features are most relevant for making predictions, aiding in feature selection and understanding the data.\n",
    "\n",
    "**(VI) Easy to implement:** The basic idea behind boosting is relatively simple to understand and implement, and many robust libraries, like scikit-learn, XGBoost, and LightGBM, offer efficient implementations.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "**(I) Overfitting**: If boosting is carried out for too many iterations or if the weak learners are too complex, it can lead to overfitting on the training data, reducing generalization performance on unseen data.\n",
    "\n",
    "**(II) Sensitivity to outliers:** While boosting can be robust to some outliers, it can still be sensitive to extreme outliers, potentially leading to suboptimal models.\n",
    "\n",
    "**(III) Computationally expensive:** Training multiple iterations of weak learners sequentially can be computationally expensive, especially for large datasets or complex models.\n",
    "\n",
    "**(III) Parameter tuning:** Boosting algorithms have hyperparameters that need to be tuned properly to achieve optimal performance, and improper tuning can lead to suboptimal results.\n",
    "\n",
    "**(V) Bias towards easy samples**: Boosting tends to focus on the most challenging data points, which can lead to less emphasis on easy-to-classify samples, potentially leading to poorer performance on those samples.\n",
    "\n",
    "**(VI) Data requirements**: Boosting may not perform well when the data is insufficient or noisy, as it relies on accurate training signals from the weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ce965-8488-4ff9-abed-b2cd29ca20e6",
   "metadata": {},
   "source": [
    "\n",
    "**Q3**. Explain how boosting works.\n",
    "\n",
    "**Answer**:\n",
    "Boosting is an ensemble learning technique that aims to improve the performance of weak learners by combining them into a strong learner. The main idea behind boosting is to train multiple weak models sequentially, where each subsequent model focuses on correcting the errors made by its predecessors. The process of boosting can be summarized as follows:\n",
    "\n",
    "**(I) Initialization**: Each data point in the training set is assigned an equal weight initially.\n",
    "\n",
    "**(II) Training weak learners:** A weak learner (e.g., decision tree, linear model, etc.) is trained on the weighted training data. The weak learner aims to focus on the data points that were misclassified by the previous weak learners.\n",
    "\n",
    "**(III) Weight update**: After training a weak learner, the weights of the misclassified data points are increased so that they have a higher influence on the next iteration. The correctly classified data points may have their weights reduced.\n",
    "\n",
    "**(IV) Reiteration:** Steps 2 and 3 are repeated for a predefined number of iterations (or until a certain threshold is reached). At each iteration, a new weak learner is trained on the updated weighted data.\n",
    "\n",
    "**(V) Aggregation**: The final prediction is made by combining the predictions of all weak learners, where each learner's contribution is weighted based on its performance during training. Typically, a weighted majority voting is used for classification tasks, and for regression tasks, the predictions are combined through weighted averaging.\n",
    "\n",
    "The process of boosting can be better understood through the following steps:\n",
    "\n",
    "Step 1: Initialize the weights for each data point in the training set. In the beginning, all data points have equal weights.\n",
    "\n",
    "Step 2: Train a weak learner (e.g., decision tree) on the weighted training data. The weak learner generates predictions for each data point.\n",
    "\n",
    "Step 3: Calculate the weighted error of the weak learner by comparing its predictions to the actual target values, considering the weights of each data point. Higher weight is given to the misclassified data points.\n",
    "\n",
    "Step 4: Based on the weighted error, calculate the contribution of the weak learner in the final model. This contribution is determined by a weight coefficient, which is proportional to the learner's performance.\n",
    "\n",
    "Step 5: Update the weights of the data points. Increase the weights of the misclassified data points, so they have a higher influence in the next iteration, and decrease the weights of the correctly classified data points.\n",
    "\n",
    "Step 6: Repeat Steps 2 to 5 for a predefined number of iterations or until a specified stopping criterion is met.\n",
    "\n",
    "Step 7: Combine the predictions of all weak learners into a final prediction. The combination can be a weighted average (regression) or a weighted majority vote (classification) of all weak learners' predictions.\n",
    "\n",
    "The boosting process continues, and the weak learners are iteratively combined, with each one focusing on the samples that were misclassified by the previous learners. This adaptive training process allows the ensemble model (the combination of all weak learners) to achieve high performance, often outperforming individual weak learners and even other ensemble techniques like bagging. The final model produced by boosting is often more accurate and robust, handling complex relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b612dc-c709-4f15-96a2-88b0b96499b6",
   "metadata": {},
   "source": [
    "**Q4**. What are the different types of boosting algorithms?\n",
    "\n",
    "**Answer**:\n",
    "There are several different types of boosting algorithms, each with its unique approach and characteristics. Some of the most popular boosting algorithms are:\n",
    "\n",
    "**(I) AdaBoost (Adaptive Boosting)**: AdaBoost is one of the earliest and most well-known boosting algorithms. It works by iteratively training weak learners, such as decision trees, on the weighted training data, and then adjusting the weights of misclassified data points to focus on the hard-to-classify samples. The final prediction is a weighted combination of all weak learners' predictions.\n",
    "\n",
    "**(II) Gradient Boosting Machines (GBM)**: Gradient Boosting Machines is a powerful boosting algorithm that builds weak learners in a sequential manner. Unlike AdaBoost, GBM optimizes the loss function of the model directly by fitting each new weak learner to the negative gradient of the loss function of the whole ensemble. It typically uses decision trees as weak learners and can handle both regression and classification problems.\n",
    "\n",
    "**(III) Extreme Gradient Boosting (XGBoost)**: XGBoost is an optimized and highly efficient implementation of Gradient Boosting Machines. It is known for its speed, scalability, and parallel processing capabilities. XGBoost incorporates regularization techniques and uses a more advanced splitting strategy to improve model accuracy and reduce overfitting.\n",
    "\n",
    "**(IV) Light Gradient Boosting Machine (LightGBM)**: LightGBM is another efficient implementation of gradient boosting that is optimized for large datasets. It uses a histogram-based approach for gradient computation, which makes it faster than traditional boosting algorithms. LightGBM also supports GPU acceleration, further enhancing its speed and scalability.\n",
    "\n",
    "**(V) CatBoost**: CatBoost is a gradient boosting algorithm that is designed to handle categorical features in the data naturally. It uses a variant of ordered boosting and incorporates various optimizations to improve performance while handling categorical variables efficiently.\n",
    "\n",
    "**(VI) Histogram-Based Boosting Algorithms**: Several boosting algorithms, including LightGBM and CatBoost, use histogram-based techniques for constructing weak learners, making them faster and more memory-efficient than traditional boosting algorithms.\n",
    "\n",
    "**(VII) LogitBoost**: LogitBoost is a boosting algorithm specifically designed for binary classification problems. It optimizes the log-likelihood loss function and updates the weights of the data points using Newton-Raphson optimization.\n",
    "\n",
    "**(VIII) LPBoost (Linear Programming Boosting)**: LPBoost is a variant of boosting that uses linear programming to find the optimal combination of weak learners. It can be applied to both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75385252-44d1-45d5-9224-b53f028e1e37",
   "metadata": {},
   "source": [
    "**Q5**. What are some common parameters in boosting algorithms?\n",
    "\n",
    "**Answer**:\n",
    "Boosting algorithms have several parameters that can be tuned to improve model performance and control the behavior of the boosting process. The specific parameters may vary depending on the algorithm implementation, but here are some common parameters found in boosting algorithms:\n",
    "\n",
    "**(I) Number of Estimators (or Boosting Rounds):** This parameter specifies the number of weak learners (estimators) to be sequentially trained during the boosting process. Increasing the number of estimators can improve model performance, but it may also lead to overfitting.\n",
    "\n",
    "**(II) Learning Rate (or Shrinkage)**: The learning rate controls the step size at each iteration when fitting weak learners. A smaller learning rate makes the model converge more slowly but can improve generalization. It is typically set to a value between 0.1 and 0.3.\n",
    "\n",
    "**(III) Max Depth (or Max Tree Depth)**: For boosting algorithms that use decision trees as weak learners, the max depth parameter determines the maximum depth of the decision trees. Setting this parameter can prevent the trees from growing too deep and overfitting.\n",
    "\n",
    "**(IV) Subsample (or Subsample Ratio)**: This parameter controls the fraction of the training data used to train each weak learner. It is common to set it to a value less than 1.0 to introduce randomness and reduce overfitting.\n",
    "\n",
    "**(V) Min Child Weight (or Min Sum Hessian)**: For gradient-based boosting algorithms, this parameter sets the minimum sum of Hessian (second derivative of the loss function) required in a child node to perform a further partition. It helps control the tree's growth and prevents overfitting.\n",
    "\n",
    "**(VI) Column Subsampling (or Feature Subsampling)**: For boosting algorithms that support feature subsampling, this parameter controls the fraction of features randomly chosen at each iteration to build weak learners. It can reduce overfitting and speed up training.\n",
    "\n",
    "**(VII) Regularization Parameters:** Many boosting algorithms support regularization techniques, such as L1 (Lasso) and L2 (Ridge) regularization. These parameters help control the complexity of the model and prevent overfitting.\n",
    "\n",
    "**(VIII) Categorical Feature Handling**: Some boosting algorithms have specific parameters to handle categorical features efficiently. They may allow different ways of encoding or handling categorical variables during training.\n",
    "\n",
    "**(IX) Early Stopping:** Early stopping is a technique used to stop the boosting process early based on the performance on a validation dataset. It helps prevent overfitting and reduces computation time.\n",
    "\n",
    "**(X) Loss Function:** For gradient-based boosting algorithms, the loss function can be specified to fit the specific task (e.g., mean squared error for regression, cross-entropy for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6439d45-d744-45b9-b8f8-07504113e837",
   "metadata": {},
   "source": [
    "**Q6**. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "\n",
    "**Answer**:\n",
    "Boosting algorithms combine weak learners to create a strong learner through an iterative process. The basic idea is to assign weights to each weak learner's predictions based on their individual performance, and then these weighted predictions are combined to make the final prediction. The combination process typically involves weighted averaging (for regression tasks) or weighted voting (for classification tasks). Here's a step-by-step explanation of how boosting algorithms combine weak learners to create a strong learner:\n",
    "\n",
    "**(I) Initialization:** In the beginning, all data points in the training set are assigned equal weights. Each weak learner is trained on this weighted training set.\n",
    "\n",
    "**(II) Training weak learners**: The first weak learner is trained on the original training data with initial weights. Subsequent weak learners are trained on updated versions of the training data, where the weights of misclassified data points from the previous iteration are increased, and the weights of correctly classified data points may be decreased.\n",
    "\n",
    "**(III) Weighting weak learners**: Each weak learner's contribution to the final prediction is determined based on its performance on the training data. Weak learners with higher accuracy or lower errors are assigned higher weights, indicating their greater influence on the final prediction.\n",
    "\n",
    "**(IV) Combining predictions**: For classification tasks, the predictions of individual weak learners are combined through a weighted majority vote. Each weak learner's prediction is multiplied by its assigned weight, and the final prediction is obtained by summing these weighted predictions and then choosing the class with the highest sum.\n",
    "\n",
    "For regression tasks, the predictions of individual weak learners are combined through a weighted average. Each weak learner's prediction is multiplied by its assigned weight, and the final prediction is obtained by summing these weighted predictions and dividing by the total weight.\n",
    "\n",
    "**(V) Reiteration:** Steps 2 to 4 are repeated for a predefined number of iterations (or until a stopping criterion is met). Each iteration introduces a new weak learner, and the boosting process focuses on the samples that were misclassified by the previous weak learners.\n",
    "\n",
    "**(VI) Final prediction:**  The final prediction is made by combining the predictions of all weak learners according to their assigned weights. The resulting model is a strong learner that combines the strengths of multiple weak learners, leading to improved accuracy and robustness compared to individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20907024-04e8-43a5-9cf2-3ec07d93cb52",
   "metadata": {},
   "source": [
    "**Q7**. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "**Answer**:\n",
    "AdaBoost (Adaptive Boosting) is one of the earliest and most popular boosting algorithms. It is an ensemble learning technique that combines multiple weak learners (often decision trees) to create a strong learner. AdaBoost is designed to improve the accuracy of the model by giving more weight to misclassified data points, thus focusing on difficult-to-classify samples in each iteration.\n",
    "\n",
    "The working of the AdaBoost algorithm can be summarized in the following steps:\n",
    "\n",
    "**(I) Initialization**: Each data point in the training set is assigned an equal weight initially. For a dataset with N samples, each data point is given an initial weight of 1/N.\n",
    "\n",
    "**(II) Training weak learners (base models)**: The first weak learner (e.g., decision tree with limited depth) is trained on the weighted training data. The weak learner is chosen such that it performs better than random guessing but still is relatively simple.\n",
    "\n",
    "**(III) Weight update**: After training the first weak learner, the algorithm evaluates its performance on the training data. Misclassified data points are assigned higher weights, while correctly classified data points may have their weights reduced. This makes the misclassified points more important in the subsequent iterations.\n",
    "\n",
    "**(IV) Training subsequent weak learners:** In the next iteration, a new weak learner is trained on the updated weighted data. The algorithm focuses on the misclassified data points from the previous iteration and tries to correctly classify them in the current iteration.\n",
    "\n",
    "**(V) Weight update and reiteration:** Steps 3 and 4 are repeated for a predefined number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is added, and the weights of data points are updated based on their classification performance in the previous iteration.\n",
    "\n",
    "**(VI) Combining weak learners:** The final prediction is made by combining the predictions of all weak learners. Each weak learner's contribution is weighted based on its performance during training. Typically, a weighted majority vote is used for classification tasks, where the final prediction is determined by the class with the highest total weighted votes. For regression tasks, a weighted average of weak learners' predictions is used.\n",
    "\n",
    "The key idea behind AdaBoost is to iteratively improve the model by giving more attention to the misclassified samples in each iteration. This adaptiveness allows AdaBoost to focus on the most challenging data points and build a strong learner that performs well on both the training data and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f8d30-487f-486f-b6b8-a52da5ac5fbe",
   "metadata": {},
   "source": [
    "**Q8**. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "**Answer**:\n",
    "In AdaBoost, the loss function used for training the weak learners (base models) is the exponential loss function, also known as the exponential error. The exponential loss function is a classification-specific loss function that is particularly well-suited for the AdaBoost algorithm.\n",
    "\n",
    "Given a binary classification problem, where the target variable takes values 1 or -1 (representing the two classes), the exponential loss function for a single data point (x_i, y_i) can be defined as:\n",
    "\n",
    "L(y_i, f(x_i)) = exp(-y_i * f(x_i))\n",
    "\n",
    "where:\n",
    "\n",
    "y_i is the true label (either 1 or -1) of the data point.\n",
    "\n",
    "f(x_i) is the prediction made by the weak learner for the data point x_i.\n",
    "\n",
    "L(y_i, f(x_i)) is the exponential loss associated with the prediction.\n",
    "\n",
    "The exponential loss function has a specific property that it heavily penalizes the misclassification of data points with high confidence. When the prediction (f(x_i)) matches the true label (y_i), the exponential loss becomes close to 0. However, when the prediction and true label are opposite (i.e., misclassification), the exponential loss increases rapidly as the confidence of the incorrect prediction grows.\n",
    "\n",
    "In the AdaBoost algorithm, the weak learners are trained to minimize the exponential loss on the weighted training data. At each iteration, the algorithm focuses on the misclassified data points from the previous iteration by increasing their weights. This way, the subsequent weak learners are encouraged to correctly classify the previously misclassified samples, effectively reducing the exponential loss in the subsequent iterations.\n",
    "\n",
    "By minimizing the exponential loss, AdaBoost adapts and improves the model with each iteration, giving more emphasis to the hard-to-classify data points. This adaptiveness is the key to the success of the AdaBoost algorithm in building a strong ensemble model from weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661c929-b6f9-400e-9ae5-57c02119871d",
   "metadata": {},
   "source": [
    "**Q9**. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "**Answer**:\n",
    "In the AdaBoost algorithm, the weights of misclassified samples are updated in such a way that the subsequent weak learners focus more on these samples in the next iteration. The updating process assigns higher weights to the misclassified samples, making them more influential in the training of the next weak learner. The weight update process can be summarized as follows:\n",
    "\n",
    "**(I) Initialization:** Each data point in the training set is assigned an equal weight initially. For a dataset with N samples, each data point is given an initial weight of 1/N.\n",
    "\n",
    "**(II) Training weak learners (base models):** The first weak learner (e.g., decision tree with limited depth) is trained on the weighted training data.\n",
    "\n",
    "**(III) Weight update:** After training the first weak learner, the algorithm evaluates its performance on the training data. The misclassified data points are identified by comparing the weak learner's predictions to the actual target labels.\n",
    "\n",
    "**(IV) Error calculation**: The weighted error (err_m) of the current weak learner (m) is calculated as the sum of the weights of misclassified samples:\n",
    "\n",
    "err_m = Î£ (weight_i) * (misclassified_i)\n",
    "\n",
    "where:\n",
    "\n",
    "weight_i is the weight of data point i.\n",
    "\n",
    "misclassified_i is an indicator function that takes the value 1 if data point i is misclassified, and 0 otherwise.\n",
    "\n",
    "**(V) Weight update formula**: The weight update for each misclassified data point (i) is given by the formula:\n",
    "\n",
    "new_weight_i = weight_i * exp(err_m)\n",
    "\n",
    "where:\n",
    "\n",
    "new_weight_i is the updated weight of data point i.\n",
    "\n",
    "weight_i is the current weight of data point i.\n",
    "\n",
    "exp() is the exponential function.\n",
    "\n",
    "**(VI) Normalization**: After updating the weights of the misclassified data points, all weights are normalized to ensure they sum up to 1. This normalization step is necessary to maintain the weights' overall meaning and keep them within a reasonable range.\n",
    "\n",
    "**(VII) Reiteration**: Steps 2 to 6 are repeated for a predefined number of iterations (or until a stopping criterion is met). In each iteration, a new weak learner is trained on the updated weighted data, and the weight update process focuses on the misclassified data points from the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a87e9b-5f53-45f9-9019-9346338a5c93",
   "metadata": {},
   "source": [
    "**Q10**. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "**Answer**:\n",
    "Increasing the number of estimators (or boosting rounds) in the AdaBoost algorithm has both positive and negative effects, and it is crucial to strike a balance to achieve optimal model performance. The number of estimators is a hyperparameter in the AdaBoost algorithm that determines how many weak learners (e.g., decision trees) will be sequentially trained during the boosting process. Here are the effects of increasing the number of estimators:\n",
    "\n",
    "**(I) Improved Training Accuracy**: One of the primary advantages of increasing the number of estimators is that it can lead to improved training accuracy. With more weak learners, the model has more opportunities to learn from the data and refine its predictions. As a result, the training accuracy tends to increase, and the model may better fit the training data.\n",
    "\n",
    "**(II) Reduced Bias:** As the number of estimators increases, the boosting process becomes more flexible and less biased. A higher number of estimators allow the model to learn more complex relationships within the data, which can help reduce underfitting and increase the model's ability to capture intricate patterns in the data.\n",
    "\n",
    "**(III) Risk of Overfitting**: While increasing the number of estimators can improve training accuracy, it can also lead to overfitting if not controlled properly. Overfitting occurs when the model becomes too specific to the training data and fails to generalize well to unseen data. The model may start memorizing the training data instead of learning the underlying patterns.\n",
    "\n",
    "**(IV) Slower Training Time**: Training additional weak learners requires more computation time, especially if each weak learner is complex or the dataset is large. As the number of estimators increases, the training time will also increase, making the model more computationally expensive.\n",
    "\n",
    "**(V) Diminishing Returns**: After a certain point, increasing the number of estimators may not result in significant improvements in performance. There is a point of diminishing returns where the model's accuracy plateaus or only improves marginally with further iterations.\n",
    "\n",
    "**(VI) Overfitting Control**: To control overfitting when increasing the number of estimators, it is common to use techniques like early stopping or cross-validation. Early stopping involves monitoring the model's performance on a validation set and stopping the boosting process when the performance starts to degrade. Cross-validation helps in selecting an optimal number of estimators that balances bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d30aa-3f62-4d3a-85ab-ebbea39eae70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
